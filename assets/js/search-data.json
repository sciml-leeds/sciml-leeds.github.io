{
  
    
        "post0": {
            "title": "SINDy - Challenge",
            "content": ". PySINDy SCiML workshop . Installing PySINDy . To install the newest version of PySINDy, you must follow the instructions on the documentation. The version available through Conda is much older than the newest release. See https://pysindy.readthedocs.io/en/latest/index.html#installation. . !pip install pysindy --quiet . [2K [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m121.0/121.0 kB[0m [31m3.1 MB/s[0m eta [36m0:00:00[0m [?25h . import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D import numpy as np from scipy.integrate import solve_ivp from sklearn.metrics import mean_squared_error import os #print(os.getcwd()) #os.chdir(&quot;../PhD/PhD/ANAET&quot;) from pysindy.utils import lorenz, lorenz_control, enzyme import pysindy as ps # bad code but allows us to ignore warnings import warnings from scipy.integrate.odepack import ODEintWarning warnings.simplefilter(&quot;ignore&quot;, category=UserWarning) warnings.simplefilter(&quot;ignore&quot;, category=FutureWarning) warnings.simplefilter(&quot;ignore&quot;, category=ODEintWarning) # Seed the random number generators for reproducibility np.random.seed(100) # Initialize integrator keywords for solve_ivp to replicate the odeint defaults integrator_keywords = {} integrator_keywords[&#39;rtol&#39;] = 1e-12 integrator_keywords[&#39;method&#39;] = &#39;LSODA&#39; integrator_keywords[&#39;atol&#39;] = 1e-12 import matplotlib as mpl mpl.rcParams[&quot;xtick.labelsize&quot;]=22 mpl.rcParams[&quot;ytick.labelsize&quot;]=22 mpl.rcParams[&quot;axes.labelsize&quot;]=26 mpl.rcParams[&#39;mathtext.fontset&#39;] = &#39;stix&#39; mpl.rcParams[&#39;font.family&#39;] = &#39;STIXGeneral&#39; . &lt;ipython-input-3-92171691bfc6&gt;:15: DeprecationWarning: Please use `ODEintWarning` from the `scipy.integrate` namespace, the `scipy.integrate.odepack` namespace is deprecated. from scipy.integrate.odepack import ODEintWarning . Starting with PySINDy . In this section, we start by using the Lorenz system for identification. The equations have the form begin{align} dot{x} &amp; = sigma (y-x), dot{y} &amp; = x( rho-z) -y, dot{z} &amp; = xy - beta z end{align} where $ sigma, rho, beta$ are scalars. The first example shows how pySINDy can be used to reproduce the results of the original paper (Brunton et al 2016). . Tasks . Run the example script below, try changing the different fitting parameters. Change the time-stepping in the training data. How does this impact the model fit? | Vary the library size and calculate the error on the test trajectory, how can you use this to select the library? | Train the lorenz equations with the parameters $(10, 8/3, 6)$ with an initial condition $[-3.65, -3.65, 5]$. Why does model identification fail for this case? | Now train the Lorenz model with the same parameters, but using the initial condition $[-8, 8, 27]$ between $t in [0, 0.5]$ for a time-step of $dt = 0.0001$. Can you successfully identify the correct equations using a second order polynomial library? | Now parameterise the Lorenz model through the bifurcation in the parameter $ rho$ | . # define the testing and training Lorenz data we will use for these examples dt = 0.001 integrator_keywords[&quot;args&quot;] = (10, 8/3, 28) ## create a training set of data t_train = np.arange(0, 10, dt) x0_train = [-8, 8, 27] t_train_span = (t_train[0], t_train[-1]) x_train = solve_ivp( lorenz, t_train_span, x0_train, t_eval=t_train, **integrator_keywords ).y.T ## create a test set of data t_test = np.arange(0, 15, dt) t_test_span = (t_test[0], t_test[-1]) x0_test = np.array([8, 7, 15]) x_test = solve_ivp( lorenz, t_test_span, x0_test, t_eval=t_test, **integrator_keywords ).y.T feature_names = [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;] #define a library feature_library = ps.PolynomialLibrary(5) sparse_regression_optimizer = ps.STLSQ(threshold=0.01, alpha =0.01) # default is lambda = 0.1 model = ps.SINDy(feature_names=feature_names, feature_library = feature_library, optimizer=sparse_regression_optimizer) model.fit(x_train, t=t_train) model.print(precision=4) prediction = model.simulate(x_test[0,:], t_test) #plot the prediction against the test data fig, ax = plt.subplots(3,1, figsize=(15,10), sharex=True, layout=&quot;constrained&quot;) for i in range(3): ax[i].plot(t_test, x_test[:,i], &quot;b&quot;) ax[i].plot(t_test, prediction[:,i], &quot;--r&quot;) ax[i].set_ylabel(feature_names[i]) ax[-1].set_xlabel(r&quot;$t$&quot;) print(&quot;mean squared error on test data&quot;, model.score(x_test, t=t_test, metric=mean_squared_error)) . (x)&#39; = -9.9998 x + 9.9998 y (y)&#39; = 27.9980 x + -0.9996 y + -0.9999 x z (z)&#39; = -2.6666 z + 1.0000 x y mean squared error on test data 4.619382264489079e-06 . . Mean field model . One reduced-order model of flow past a cylinder is the coupled system of equations begin{align} dot{x} &amp;= mu x- omega y - xz, dot{y} &amp;= mu y + omega x -yz, dot{z} &amp; = -z + x^2 + y^2 end{align} where $ mu$ and $ omega$ are scalars. $ omega$ gives the frequency of oscillation. . Tasks: . Generate a training set of data using the initial conditions $[0.01, 0.01, 0.1]$, $ omega=1$ and $ mu=0.1$. Use SINDy to identify the model. | Now fit a SINDy model to data only when the model is on the stable limit cycle (the data is only oscillating). What happens to the model? | mu = 0.1 omega =1 A=-1 lam = 1 def cylinder_wake(t, xv): x,y,z = xv return [mu*x-omega*y+A*x*z, omega*x + mu*y + A*y*z, -z+x**2+y**2] y0 = [0.01, 0.01, 0.1] . Coding challenge, identifying a model with noise . In this exercise, we try to improve the fit of SINDy in the presence of noise. Noise presents a substantial challenge when we are fitting as we have to take the derivative of the data. You can use use a second order library. . Tasks . Compare the time series of $x,y,z$ with their derivatives, when $10 %$ noise is added | By using a different method, try to identify a model with $10 %$ noise added. | Now try with $50 %$ noise added. Is it possible to identify a model? | You are also given that the data satisfies the symmetry $x rightarrow -x, y rightarrow -y, z rightarrow z$. . integrator_keywords[&quot;args&quot;] = (10, 8/3, 28) t_train = np.arange(0, 10, dt) t_train_span = [t_train[0], t_train[-1]] labels = [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;] x_train = solve_ivp( lorenz, t_train_span, x0_train, t_eval=t_train, **integrator_keywords ).y.T #calculate the standard deviation from zero mean rmse = mean_squared_error(x_train, np.zeros(x_train.shape), squared=False) #add noise to the measurement data x_train_noisy = x_train + np.random.normal(0, rmse / 50, x_train.shape) xdot_noisy = ps.FiniteDifference()._differentiate(x_train_noisy, t_train) t_test = np.arange(0, 15, dt) t_test_span = (t_test[0], t_test[-1]) x0_test = np.array([8, 7, 15]) x_test = solve_ivp( lorenz, t_test_span, x0_test, t_eval=t_test, **integrator_keywords ).y.T fig, axs = plt.subplots(1,3, figsize=(15, 5), layout=&quot;constrained&quot;) for i in range(3): axs[i].plot(t_train, x_train_noisy[:,i], &quot;b&quot;) axs[i].set_ylabel(rf&quot;${labels[i]}$&quot;) axs[i].set_xlabel( r&quot;$t$&quot;) fig, axs = plt.subplots(1,3, figsize=(15,5), layout=&quot;constrained&quot;) for i in range(3): axs[i].plot(t_train, xdot_noisy[:,i], &quot;b&quot;) axs[i].set_ylabel(r&quot;$ dot}$&quot;.format(labels[i])) axs[i].set_xlabel(r&quot;$t$&quot;) fig, axs = plt.subplots(2,1, figsize=(8,8), layout=&quot;constrained&quot;, sharex=True) axs[0].plot(t_train, x_train_noisy[:,0], &quot;b&quot;) axs[1].plot(t_train, xdot_noisy[:,0], &quot;b&quot;) axs[1].set_xlabel(r&quot;$t$&quot;) axs[0].set_ylabel(r&quot;$x$&quot;) axs[1].set_ylabel(r&quot;$ dot{x}$&quot;) xx=x_train[:,0] yy=x_train[:,1] zz=x_train[:,2] fig = plt.figure(figsize=(10,10)) ax3d = fig.add_subplot(111, projection=&#39;3d&#39;) ax3d.plot(xx,yy,zz, &quot;k&quot;) ax3d.scatter(xx[0], yy[0], zz[0], c=&quot;r&quot;, label=&quot;initial condition&quot;) ax3d.set_xlabel(xlabel=&quot;x&quot;, fontsize=18) ax3d.set_ylabel(ylabel=&quot;y&quot;, fontsize=18) ax3d.set_zlabel(zlabel=&quot;z&quot;, fontsize=18) ax3d.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=14) plt.legend(loc=&quot;upper right&quot;, fontsize=18)# . &lt;matplotlib.legend.Legend at 0x797f06f86d40&gt; . . . . . .",
            "url": "https://sciml-leeds.github.io/workshop/sindy/2023/10/27/SINDy_Challenges.html",
            "relUrl": "/workshop/sindy/2023/10/27/SINDy_Challenges.html",
            "date": " ‚Ä¢ Oct 27, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Reinforcement learning introduction - Solution",
            "content": ". Reinforcement learning introduction: Frozen Lake example . The Frozen Lake example is a classic reinforcement learning problem, where the goal is to teach an agent to navigate a frozen lake and reach the goal without falling through the ice. . . In this example, we use the Q-learning algorithm to train an agent to navigate the FrozenLake environment. The Q-learning algorithm works by learning a Q-table, which is a table that maps each state-action pair to a value that represents the expected future reward of taking that action in that state. The Q-table is initialized to zeros, and is updated over time based on the rewards that the agent receives for taking actions in different states. . The training process involves repeatedly running episodes, where each episode consists of the agent taking actions in the environment until it reaches the goal or falls in a hole. During each time step of an episode, the agent selects an action based on the current state and the Q-table, and then takes that action and observes the resulting reward and next state. The Q-table is then updated based on the observed reward and the expected future reward of the next state, according to the Q-learning update rule. . Once the agent has been trained, we test its performance on a set of test episodes. During each test episode, the agent takes actions in the environment using the Q-table that was learned during training, and we observe whether it is able to reach the goal or not. . Overall, the Frozen Lake example is a simple but illustrative example of how reinforcement learning can be used to train an agent to navigate an environment and accomplish a task. . . . . . . . . Description: . The Frozen Lake environment is a grid of frozen ice, holes, and a goal, here we will have (4*4) grid size, that‚Äôs mean we will have (16 STATES) each cell repersent a state. . The agent starts at the top left corner cell in the grid, and can take four actions at each time step, We will have (4 ACTIONS): . move up | move down | move left | move right | . The goal is to reach the goal cell in the bottom right corner of the grid without falling through any holes. Therefore, our REWARDS are: . If agent current state is a hole state, the reward = -1 | If agent current state is the goal state, the reward = +1 | Else, the reward = 0 | . . WE‚ÄôRE GOING TO DO TWO VERSIONS of FROZEN LAKE EXAMPLE: . . Version [1]: . In this version we use ‚ÄòGym‚Äô to simplfiy the code . Import packages and setup your environment . This code imports the FrozenLake environment from the OpenAI Gym library and creates an instance of the environment. . import gym import numpy as np import matplotlib.pyplot as plt # Create the FrozenLake environment env = gym.make(&#39;FrozenLake-v1&#39;) . This code initializes the Q-table to zeros. The Q-table is a matrix where the rows represent the possible states of the environment and the columns represent the possible actions that the agent can take. . # Initialize the Q-table to zeros Q = np.zeros([env.observation_space.n, env.action_space.n]) . Hyperparameters üìà . # Set hyperparameters lr = 0.8 # learning rate gamma = 0.95 # discount factor num_episodes = 2000 # number of training episodes . TRAIN YOUR AGENT ü§ñ ‚ùÑ . This code trains the agent using Q-learning. During training, the agent interacts with the environment by selecting actions based on the Q-table and updating the Q-table based on the observed reward. The hyperparameters lr and gamma control how much the agent values immediate rewards versus future rewards. The num_episodes parameter controls how many times the agent interacts with the environment. . # Keep track of the total reward for each episode rewards = np.zeros(num_episodes) # Train the agent using Q-learning for i in range(num_episodes): # Reset the environment for each episode s = env.reset() done = False while not done: # Choose an action based on the Q-table, with some random noise a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i+1))) # Take the chosen action and observe the next state and reward s_new, r, done, _ = env.step(a) # Update the Q-table based on the observed reward Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s_new,:]) - Q[s,a]) # Add the reward to the total reward for the episode rewards[i] += r # Set the current state to the next state s = s_new . TEST YOUR AGENT üß™ . This code tests the agent on 100 episodes after training. During testing, the agent chooses actions based on the Q-table and tries to reach the goal state. The code keeps track of the number of successful episodes (where the agent reaches the goal state) and prints the success rate at the end. . # Test the agent on 100 episodes num_successes = 0 for i in range(100): s = env.reset() done = False while not done: # Choose an action based on the Q-table a = np.argmax(Q[s,:]) s, r, done, _ = env.step(a) if r == 1: num_successes += 1 # Print the success rate print(&quot;Success rate:&quot;, num_successes/100) . Success rate: 0.54 . NOW LET‚Äôs PLOT THE LEARNING üñå . # Calculate the rolling average of rewards rolling_avg_rewards = np.zeros(num_episodes) window_size = 100 for i in range(num_episodes): rolling_avg_rewards[i] = np.mean(rewards[max(0,i-window_size+1):(i+1)]) # Plot the total rewards and rolling average rewards fig, ax = plt.subplots(2, 1, figsize=(8,8)) ax[0].plot(rewards) ax[0].set_xlabel(&#39;Episode&#39;) ax[0].set_ylabel(&#39;Total reward&#39;) ax[1].plot(rolling_avg_rewards) ax[1].set_xlabel(&#39;Episode&#39;) ax[1].set_ylabel(f&#39;Rolling average reward (window size {window_size})&#39;) . Text(0, 0.5, ‚ÄòRolling average reward (window size 100)‚Äô) . . . . . . . Version [2] . . LET GET MORE TO THE DETAILS: . . Import packages . import numpy as np import random . First, we define the FrozenLake environment as a class, with methods for resetting the environment, taking actions, rendering the current state, and showing the current Q-table and policy. . class FrozenLake: def __init__(self, size=4): self.size = size self.grid = np.zeros((size, size), dtype=int) self.start_state = (0, 0) self.goal_state = (size-1, size-1) self.hole_states = [(1, 1), (2, 3), (3, 0)] for i, j in self.hole_states: self.grid[i][j] = 1 def reset(self): self.current_state = self.start_state return self.current_state def step(self, action): i, j = self.current_state if action == 0: # move up i = max(i-1, 0) elif action == 1: # move down i = min(i+1, self.size-1) elif action == 2: # move left j = max(j-1, 0) elif action == 3: # move right j = min(j+1, self.size-1) self.current_state = (i, j) if self.current_state == self.goal_state: reward = 1 done = True elif self.current_state in self.hole_states: reward = -1 done = True else: reward = 0 done = False return self.current_state, reward, done # console text printing the grid and show the agent&#39;s moves def render(self): print(&#39; n&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: if (i, j) == self.current_state: print(&#39;S&#39;, end=&#39; &#39;) elif (i, j) == self.goal_state: print(&#39;G&#39;, end=&#39; &#39;) else: print(&#39;.&#39;, end=&#39; &#39;) elif self.grid[i][j] == 1: if (i, j) == self.current_state: print(&#39;S&#39;, end=&#39; &#39;) else: print(&#39;X&#39;, end=&#39; &#39;) print() print() #print the Q-table of all values def show_q_table(self, q_table): print(&#39;--&#39;) print(&#39;Q-Table:&#39;) print(&#39;--&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: print( &#39;%.2f&#39; % q_table[i][j][0], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][1], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][2], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][3]) else: print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;) print() # In one text line show the policy (the sequence of actions that agent take ) def show_policy(self, q_table): print(&#39; n Policy:&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: action = np.argmax(q_table[i][j]) if action == 0: print(&#39;UP&#39;, end=&#39; &#39;) elif action == 1: print(&#39;DOWN&#39;, end=&#39; &#39;) elif action == 2: print(&#39;LEFT&#39;, end=&#39; &#39;) elif action == 3: print(&#39;RIGHT&#39;, end=&#39; &#39;) else: print(&#39;STAY&#39;, end=&#39; &#39;) . Next, we create an instance of the environment and initialize the Q-table with zeros. . # Create the environment env = FrozenLake() # Initialize Q-table with zeros q_table = np.zeros((env.size, env.size, 4)) . Hyperparameters üìà . We then set some hyperparameters for the Q-learning algorithm, such as the number of episodes to run, the maximum number of steps per episode, the learning rate, the discount factor, the starting exploration rate (epsilon), the minimum exploration rate, and the rate at which epsilon decays over time. . # Set hyperparameters num_episodes = 10000 max_steps_per_episode = 100 learning_rate = 0.1 discount_factor = 0.99 epsilon = 1.0 min_epsilon = 0.01 epsilon_decay_rate = 0.001 . We define an epsilon-greedy policy for selecting actions, which chooses a random action with probability epsilon or the greedy action (i.e., the action with the highest Q-value) with probability 1 - epsilon. . . # Define epsilon-greedy policy def epsilon_greedy_policy(state): if random.uniform(0, 1) &lt; epsilon: return random.randint(0, 3) else: return np.argmax(q_table[state[0]][state[1]]) . We train the agent by running a loop over the specified number of episodes. In each episode, we start by resetting the environment and selecting actions according to the epsilon-greedy policy. We then update the Q-values for the current state-action pair using the Q-learning update rule. Finally, we update the current state and repeat until the episode ends (either because the agent reaches the goal or exceeds the maximum number of steps). . We decay the exploration rate (epsilon) after each episode to gradually shift the agent from exploration to exploitation over time. . Periodically, we render the current state of the environment and display the current Q-table and policy for visualization. . # Train agent for episode in range(num_episodes): state = env.reset() done = False t = 0 while not done and t &lt; max_steps_per_episode: action = epsilon_greedy_policy(state) next_state, reward, done = env.step(action) q_table[state[0]][state[1]][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action]) state = next_state t += 1 epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay_rate)) # Show progress if episode % 1000 == 0: env.render() env.show_q_table(q_table) env.show_policy(q_table) . . . . . . S . . . . . X X . . G -- Q-Table: -- 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.10 NULL NULL NULL NULL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Policy: UP UP UP UP UP STAY UP UP UP UP UP STAY STAY UP UP UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.84 0.85 0.96 0.82 0.94 0.58 0.85 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.65 -0.95 0.97 0.80 0.47 -1.00 0.32 0.81 -0.95 0.72 0.45 0.98 0.97 0.99 0.96 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.58 0.71 -0.81 0.98 0.98 0.99 0.94 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.88 0.91 0.96 0.90 0.94 0.69 0.88 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.91 -0.95 0.79 0.66 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.65 0.84 -0.88 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.90 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . Once training is complete, we test the agent by running a loop until the agent reaches the goal or exceeds the maximum number of steps. In each step, we select the greedy action (i.e., the action with the highest Q-value) and update the current state. We render the environment at each step for visualization. . # Test agent state = env.reset() done = False while not done: action = np.argmax(q_table[state[0]][state[1]]) next_state, reward, done = env.step(action) env.render() state = next_state . . S . . . X . . . . . X X . . G . . S . . X . . . . . X X . . G . . . . . X S . . . . X X . . G . . . . . X . . . . S X X . . G . . . . . X . . . . . X X . S G . . . . . X . . . . . X X . . S . .",
            "url": "https://sciml-leeds.github.io/workshop/reinforcement_learning/2023/05/05/Reinforcement_learning_Solution.html",
            "relUrl": "/workshop/reinforcement_learning/2023/05/05/Reinforcement_learning_Solution.html",
            "date": " ‚Ä¢ May 5, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Reinforcement learning introduction - Challenge",
            "content": ". Reinforcement learning introduction: Frozen Lake example . The Frozen Lake example is a classic reinforcement learning problem, where the goal is to teach an agent to navigate a frozen lake and reach the goal without falling through the ice. . . Description: . The Frozen Lake environment is a grid of frozen ice, holes, and a goal, here we will have (4*4) grid size, that‚Äôs mean we will have (16 STATES) each cell represent a state. . The agent starts at the top left corner cell in the grid, and can take four actions at each time step, We will have (4 ACTIONS): . move up | move down | move left | move right | . The goal is to reach the goal cell in the bottom right corner of the grid without falling through any holes. Therefore, our REWARDS are: . If agent current state is a hole state, the reward = -1 | If agent current state is the goal state, the reward = +1 | Else, the reward = 0 | . . . . Import packages and setup your environment . import numpy as np import random . First, we define the FrozenLake environment as a class, with methods for resetting the environment, taking actions, rendering the current state, and showing the current Q-table and policy. . class FrozenLake: def __init__(self, size=4): self.size = size self.grid = np.zeros((4, 4), dtype=int) # spcifiy grid size please self.start_state = (0, 0) self.goal_state = (3, 3) # can you state the goal state in the grid (row, column) self.hole_states = [(1, 1), (2, 3), (3, 0)] for i, j in self.hole_states: self.grid[i][j] = 1 def reset(self): self.current_state = self.start_state return self.current_state def step(self, action): # could you please set the actions as numbers i, j = self.current_state if action == 0: # move up i = max(i-1, 0) elif action == 1: # move down i = min(i+1, self.size-1) elif action == 2: # move left j = max(j-1, 0) elif action == 3: # move right j = min(j+1, self.size-1) self.current_state = (i, j) # what is your rewads? if self.current_state == self.goal_state: reward = 1 # winning reward? done = True elif self.current_state in self.hole_states: reward = -1 # losing for hole states, reward? done = True else: reward = 0 # else? done = False return self.current_state, reward, done def render(self): print(&#39; n&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: if (i, j) == self.current_state: print(&#39;S&#39;, end=&#39; &#39;) elif (i, j) == self.goal_state: print(&#39;G&#39;, end=&#39; &#39;) else: print(&#39;.&#39;, end=&#39; &#39;) elif self.grid[i][j] == 1: if (i, j) == self.current_state: print(&#39;S&#39;, end=&#39; &#39;) else: print(&#39;X&#39;, end=&#39; &#39;) print() print() def show_q_table(self, q_table): print(&#39;--&#39;) print(&#39;Q-Table:&#39;) print(&#39;--&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: print( &#39;%.2f&#39; % q_table[i][j][0], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][1], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][2], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][3]) else: print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;) print() # In one text line show the policy (the sequence of actions that agent take ) def show_policy(self, q_table): print(&#39; n Policy:&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: action = np.argmax(q_table[i][j]) if action == 0: print(&#39;UP&#39;, end=&#39; &#39;) elif action == 1: print(&#39;DOWN&#39;, end=&#39; &#39;) elif action == 2: print(&#39;LEFT&#39;, end=&#39; &#39;) elif action == 3: print(&#39;RIGTH&#39;, end=&#39; &#39;) else: print(&#39;STAY&#39;, end=&#39; &#39;) . Next, we create an instance of the environment and initialize the Q-table with zeros. . env = FrozenLake() q_table = np.zeros((env.size, env.size, 4)) . Hyperparameters . We then set some hyperparameters for the Q-learning algorithm, such as the number of episodes to run, the maximum number of steps per episode, the learning rate, the discount factor, the starting exploration rate (epsilon), the minimum exploration rate, and the rate at which epsilon decays over time. . # Could you please set your hyperparameters? num_episodes = 2000 max_steps_per_episode = 10 learning_rate = 0.05 discount_factor = 0.99 epsilon = 1.0 min_epsilon = 0.01 epsilon_decay_rate = 0.001 . We define an epsilon-greedy policy for selecting actions, which chooses a random action with probability epsilon or the greedy action (i.e., the action with the highest Q-value) with probability 1 - epsilon. . . # Define epsilon-greedy policy def epsilon_greedy_policy(state): if random.uniform(0, 1) &lt; epsilon: return random.randint(0, 3) else: return np.argmax(q_table[state[0]][state[1]]) . We train the agent by running a loop over the specified number of episodes. In each episode, we start by resetting the environment and selecting actions according to the epsilon-greedy policy. We then update the Q-values for the current state-action pair using the Q-learning update rule. Finally, we update the current state and repeat until the episode ends (either because the agent reaches the goal or exceeds the maximum number of steps). . We decay the exploration rate (epsilon) after each episode to gradually shift the agent from exploration to exploitation over time. . Periodically, we render the current state of the environment and display the current Q-table and policy for visualization. . for episode in range(num_episodes): state = env.reset() done = False t = 0 while not done and t &lt; max_steps_per_episode: action = epsilon_greedy_policy(state) next_state, reward, done = env.step(action) # what is missing to update the Q value? #q_table[state[0]][state[1]][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action]) q_table[state[0]][state[1]][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action]) state = next_state t += 1 epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay_rate)) # Show progress if episode % 1000 == 0: env.render() env.show_q_table(q_table) env.show_policy(q_table) . . . . . . S . . . . . X X . . G -- Q-Table: -- 0.00 0.00 0.00 0.00 0.00 -0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 NULL NULL NULL NULL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Policy: UP UP UP UP UP STAY UP UP UP UP UP STAY STAY UP UP UP . . . . . S . . . . . X X . . G -- Q-Table: -- 0.00 0.00 0.00 0.00 0.00 -1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.00 NULL NULL NULL NULL 0.00 0.00 -0.64 0.00 0.00 -0.19 0.00 0.00 0.00 -0.74 0.00 0.00 -0.26 0.00 0.00 0.00 0.00 0.00 0.00 -0.05 NULL NULL NULL NULL NULL NULL NULL NULL 0.00 0.00 -0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Policy: UP UP UP UP UP STAY UP UP UP DOWN UP STAY STAY UP UP UP Once training is complete, we test the agent by running a loop until the agent reaches the goal or exceeds the maximum number of steps. In each step, we select the greedy action (i.e., the action with the highest Q-value) and update the current state. We render the environment at each step for visualization. . # Test agent state = env.reset() done = False while not done: action = np.argmax(q_table[state[0]][state[1]]) next_state, reward, done = env.step(action) env.render() state = next_state . S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G . .",
            "url": "https://sciml-leeds.github.io/workshop/reinforcement_learning/2023/05/05/Reinforcement_learning_Challenges.html",
            "relUrl": "/workshop/reinforcement_learning/2023/05/05/Reinforcement_learning_Challenges.html",
            "date": " ‚Ä¢ May 5, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://sciml-leeds.github.io/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://sciml-leeds.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://sciml-leeds.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Events",
          "content": "Below is an overview of events organised by the SciML community. We aim to put recordings of each event online within a week of them taking place. . Please get in touch if you‚Äôd like to come to talk to us, we‚Äôre a very diverse group in terms of applications in scientific machine learning and would to hear about your work! . Seminars . Title Speaker Date Media . Rapid Spatio-Temporal Flood Modeling - Hydraulic GNN Approach | Roberto Bentivoglio (Delft University of Technology) | 16/02/2023 11:00 | recording | . Forecasting Global Weather with Graph Neural Networks | Ryan Keisler (KoBold Metals) | 10/11/2023 15:00 | recording | . Could deep learning methods replace numerical weather models? | Mariana Clare (European Centre for Medium Range Weather Forecasts) | 20/10/2023 15:00 | recording | . Feature-Preserving Point Cloud Simplification with Gaussian Processes | Thomas. M. McDonald (University of Manchester) | 14/07/2023 14:00 | recording | . Physics-based domain adaptation for dynamical systems forecasting | Zack Xuereb Conti (Alan Turing Institute) | 19/05/2023 14:00 | recording | . ClimaX: A foundation model for weather and climate | Tung Nguyen (UCLA) | 17/03/2023 15:00 | recording | . Sea ice detection from concurrent visible and SAR imagery using a convolutional neural network | Martin Rogers (British Antarctic Survey) | 24/02/2023 11:00 | recording | . Physics-informed Machine Learning for Trustworthy Climate Emulators | Bj√∂rn L√ºtjens (MIT) | 10/02/2023 14:00 | recording | . Explainable AI for identifying regional climate change patterns | Zack Labe (Princeton) | 13/01/2023 14:00 | recording | . Extending the capabilities of data-driven reduced-order models to make predictions for unseen scenarios | Claire Heaney (Imperial College) | 18/11/2022 11:00 | recording | . Workshops . Title Speaker Date Media . Physics-informed neural networks (PINNs) | Fergus Shone (University of Leeds) | 17/02/2023 14:00 | recording | . Reinforcement Learning | Alhanof Alolyan (University of Leeds) | 17/04/2023 14:00 | code | . SINDy | Alasdair Roy (University of Leeds) | 27/10/2023 14:00 | recording code | .",
          "url": "https://sciml-leeds.github.io/events/",
          "relUrl": "/events/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "GPU platforms",
          "content": "Below is an overview of GPU platforms available for research by staff at the University of Leeds: . GPU platforms . Name Hardware Access Project . ARC3 | 2 nodes with 2 x NVIDIA K80s, 6 nodes with 4 x NVIDIA P100s | https://arcdocs.leeds.ac.uk/getting_started/request_hpc_acct.html | ARC4 | . JASMIN | 6x NVidia GV100GL across 3 nodes | https://www.jasmin.ac.uk | | . DiRAC (Tursa) | 144 nodes with 4x Nvidia RedStone A100-40 each | https://dirac.ac.uk/community/#AccessingDiRAC | | . MAGEO | 5 NVIDIA DGX-1 MaxQ nodes | Via email (see website) | | . BEDE | 38x Nvidia V100 across 38 nodes in cluster | EPSRC funded projects (pre EPSRC-application access available) | | . JADE II | 63 DGX MAX-Q nodes with 8x NVidia V100 each | | | . LEARN | 1 DGX A100 | | | . Google Colab | NVidia k80s | https://cloud.google.com/gpu/ | | . Kaggle Notebooks | 30 GPU hrs/week | https://www.kaggle.com/code/dansbecker/running-kaggle-kernels-with-a-gpu | | . Gradient | M4000 in free tier | Free or paid tiers | | . Graphcore | Graphcore IPU-POD16 | Free during evaluation period | | . EuroHPC JU - supercomputers | .",
          "url": "https://sciml-leeds.github.io/gpu/",
          "relUrl": "/gpu/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://sciml-leeds.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}